{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_tickers = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'BRK-B', 'V', 'PYPL', 'MA',\n",
    "    'BABA', 'JNJ', 'WMT', 'NFLX', 'DIS', 'NVDA', 'HD', 'PFE', 'ADBE', 'INTC', 'VZ', 'CSCO',\n",
    "    'GS', 'BA', 'CVX', 'UNH', 'KO', 'XOM', 'PEP', 'MCD', 'NKE', 'IBM', 'GM', 'GE', 'INTU',\n",
    "    'TXN', 'WFC', 'AMT', 'LMT', 'SPY', 'MS', 'GS', 'CAT', 'T', 'AXP', 'COP', 'MDT', 'MMM',\n",
    "    'UPS', 'PLD', 'AMD', 'F', 'PFE', 'GILD', 'C', 'STZ', 'LVS', 'MGM', 'ZTS', 'BMY', 'DELL',\n",
    "    'TMO', 'LLY', 'AMGN', 'DUK', 'STT', 'BA', 'AIG', 'CL', 'OXY', 'MU', 'SLB', 'HCA', 'HPE',\n",
    "    'LUV', 'UAL', 'SPG', 'CME', 'CSX', 'ETN', 'FIS', 'REGN', 'CVS', 'MDLZ', 'SYK', 'WBA',\n",
    "    'ZBRA', 'NOC', 'RMD', 'CHTR', 'TROW', 'RJF', 'FOXA', 'FOXF', 'HSY', 'KHC', 'NKE', 'TRV',\n",
    "    'ADP', 'VFC', 'MSCI', 'MKC', 'BAX', 'FISV', 'LRCX', 'TGT', 'FIS', 'RTX', 'CDNS', 'DG',\n",
    "    'PEAK', 'LPLA', 'EXC', 'CTSH', 'MTB', 'STZ', 'WFC', 'COF', 'DHR', 'SIVB', 'CCL', 'ECL',\n",
    "    'SPGI', 'TEL', 'WST', 'FMC', 'EL', 'VRSN', 'SWK', 'TMO', 'DE', 'ADBE', 'JNJ', 'SBUX', 'PFE',\n",
    "    'HPE', 'MMM', 'ABT', 'INTU', 'MELI', 'BA', 'GOOG'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  127 of 127 completed\n",
      "\n",
      "3 Failed downloads:\n",
      "['SIVB', 'FISV', 'PEAK']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "C:\\Users\\wjlwi\\AppData\\Local\\Temp\\ipykernel_32728\\836044412.py:10:FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "def fetch_stock_data(tickers, start_date=\"2020-01-01\", end_date=\"2025-02-13\"):\n",
    "    etf_data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "    return etf_data\n",
    "\n",
    "# Get the data for the ETFs\n",
    "stock_data = fetch_stock_data(top_100_tickers)\n",
    "\n",
    "# Calculate the daily price change (percentage change) for each stock ticker\n",
    "stock_price_changes = stock_data.pct_change().dropna(how='all')\n",
    "rolling_volatility = stock_price_changes.rolling(window=3).std()\n",
    "rolling_volatility.to_csv(\"stock_data_vol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AAPL (1/143)...\n",
      "Processing MSFT (2/143)...\n",
      "Processing GOOGL (3/143)...\n",
      "Processing AMZN (4/143)...\n",
      "Processing TSLA (5/143)...\n",
      "Processing META (6/143)...\n",
      "Processing NVDA (7/143)...\n",
      "Processing BRK-B (8/143)...\n",
      "Processing V (9/143)...\n",
      "Processing PYPL (10/143)...\n",
      "Processing MA (11/143)...\n",
      "Processing BABA (12/143)...\n",
      "Processing JNJ (13/143)...\n",
      "Processing WMT (14/143)...\n",
      "Processing NFLX (15/143)...\n",
      "Processing DIS (16/143)...\n",
      "Processing NVDA (17/143)...\n",
      "Processing HD (18/143)...\n",
      "Processing PFE (19/143)...\n",
      "Processing ADBE (20/143)...\n",
      "Processing INTC (21/143)...\n",
      "Processing VZ (22/143)...\n",
      "Processing CSCO (23/143)...\n",
      "Processing GS (24/143)...\n",
      "Processing BA (25/143)...\n",
      "Processing CVX (26/143)...\n",
      "Processing UNH (27/143)...\n",
      "Processing KO (28/143)...\n",
      "Processing XOM (29/143)...\n",
      "Processing PEP (30/143)...\n",
      "Processing MCD (31/143)...\n",
      "Processing NKE (32/143)...\n",
      "Processing IBM (33/143)...\n",
      "Processing GM (34/143)...\n",
      "Processing GE (35/143)...\n",
      "Processing INTU (36/143)...\n",
      "Processing TXN (37/143)...\n",
      "Processing WFC (38/143)...\n",
      "Processing AMT (39/143)...\n",
      "Processing LMT (40/143)...\n",
      "Processing SPY (41/143)...\n",
      "Processing MS (42/143)...\n",
      "Processing GS (43/143)...\n",
      "Processing CAT (44/143)...\n",
      "Processing T (45/143)...\n",
      "Processing AXP (46/143)...\n",
      "Processing COP (47/143)...\n",
      "Processing MDT (48/143)...\n",
      "Processing MMM (49/143)...\n",
      "Processing UPS (50/143)...\n",
      "Processing PLD (51/143)...\n",
      "Processing AMD (52/143)...\n",
      "Processing F (53/143)...\n",
      "Processing PFE (54/143)...\n",
      "Processing GILD (55/143)...\n",
      "Processing C (56/143)...\n",
      "Processing STZ (57/143)...\n",
      "Processing LVS (58/143)...\n",
      "Processing MGM (59/143)...\n",
      "Processing ZTS (60/143)...\n",
      "Processing BMY (61/143)...\n",
      "Processing DELL (62/143)...\n",
      "Processing TMO (63/143)...\n",
      "Processing LLY (64/143)...\n",
      "Processing AMGN (65/143)...\n",
      "Processing DUK (66/143)...\n",
      "Processing STT (67/143)...\n",
      "Processing BA (68/143)...\n",
      "Processing AIG (69/143)...\n",
      "Processing CL (70/143)...\n",
      "Processing OXY (71/143)...\n",
      "Processing MU (72/143)...\n",
      "Processing SLB (73/143)...\n",
      "Processing HCA (74/143)...\n",
      "Processing HPE (75/143)...\n",
      "Processing LUV (76/143)...\n",
      "Processing UAL (77/143)...\n",
      "Processing SPG (78/143)...\n",
      "Processing CME (79/143)...\n",
      "Processing CSX (80/143)...\n",
      "Processing ETN (81/143)...\n",
      "Processing FIS (82/143)...\n",
      "Processing REGN (83/143)...\n",
      "Processing CVS (84/143)...\n",
      "Processing MDLZ (85/143)...\n",
      "Processing SYK (86/143)...\n",
      "Processing WBA (87/143)...\n",
      "Processing ZBRA (88/143)...\n",
      "Processing NOC (89/143)...\n",
      "Processing RMD (90/143)...\n",
      "Processing CHTR (91/143)...\n",
      "Processing TROW (92/143)...\n",
      "Processing RJF (93/143)...\n",
      "Processing FOXA (94/143)...\n",
      "Processing FOXF (95/143)...\n",
      "Processing HSY (96/143)...\n",
      "Processing KHC (97/143)...\n",
      "Processing NKE (98/143)...\n",
      "Processing TRV (99/143)...\n",
      "Processing ADP (100/143)...\n",
      "Processing VFC (101/143)...\n",
      "Processing MSCI (102/143)...\n",
      "Processing MKC (103/143)...\n",
      "Processing BAX (104/143)...\n",
      "Processing FISV (105/143)...\n",
      "Processing LRCX (106/143)...\n",
      "Processing TGT (107/143)...\n",
      "Processing FIS (108/143)...\n",
      "Processing RTX (109/143)...\n",
      "Processing CDNS (110/143)...\n",
      "Processing DG (111/143)...\n",
      "Processing PEAK (112/143)...\n",
      "Processing LPLA (113/143)...\n",
      "Processing EXC (114/143)...\n",
      "Processing CTSH (115/143)...\n",
      "Processing MTB (116/143)...\n",
      "Processing STZ (117/143)...\n",
      "Processing WFC (118/143)...\n",
      "Processing COF (119/143)...\n",
      "Processing DHR (120/143)...\n",
      "Processing SIVB (121/143)...\n",
      "Processing CCL (122/143)...\n",
      "Processing ECL (123/143)...\n",
      "Processing SPGI (124/143)...\n",
      "Processing TEL (125/143)...\n",
      "Processing WST (126/143)...\n",
      "Processing FMC (127/143)...\n",
      "Processing EL (128/143)...\n",
      "Processing VRSN (129/143)...\n",
      "Processing SWK (130/143)...\n",
      "Processing TMO (131/143)...\n",
      "Processing DE (132/143)...\n",
      "Processing ADBE (133/143)...\n",
      "Processing JNJ (134/143)...\n",
      "Processing SBUX (135/143)...\n",
      "Processing PFE (136/143)...\n",
      "Processing HPE (137/143)...\n",
      "Processing MMM (138/143)...\n",
      "Processing ABT (139/143)...\n",
      "Processing INTU (140/143)...\n",
      "Processing MELI (141/143)...\n",
      "Processing BA (142/143)...\n",
      "Processing GOOG (143/143)...\n"
     ]
    }
   ],
   "source": [
    "# def generate_ticker_sentiment_df(ticker):\n",
    "#     res_url = f'https://finance.yahoo.com/rss/headline?s={ticker}'\n",
    "#     feed = feedparser.parse(res_url)\n",
    "#     keyword = ticker.lower()\n",
    "#     data = []\n",
    "#     for entry in feed.entries:\n",
    "#         if keyword not in entry.summary.lower():\n",
    "#             continue\n",
    "        \n",
    "#         try:\n",
    "#             published_date = entry['published']\n",
    "#             content = entry.summary\n",
    "#             # sentiment = pipe(entry.summary)[0]\n",
    "#             # return(entry)\n",
    "#             data.append({\n",
    "#                 \"Ticker\": ticker,\n",
    "#                 \"published_date\": published_date,\n",
    "#                 \"Content\": content\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error analyzing sentiment for {ticker}: {e}\")\n",
    "#             continue\n",
    "#     sentiment_df = pd.DataFrame(data)\n",
    "#     return sentiment_df\n",
    "\n",
    "# def generate_stock_df(stock_tickers):\n",
    "#     sentiment_data = []\n",
    "\n",
    "#     # Loop through the list of tickers\n",
    "#     for i, ticker in enumerate(stock_tickers):  # Assuming 'Ticker' column in DataFrame\n",
    "#         print(f\"Processing {ticker} ({i + 1}/{len(stock_tickers)})...\")\n",
    "        \n",
    "#         # Generate sentiment data for each ticker\n",
    "#         ticker_sentiment = generate_ticker_sentiment_df(ticker)\n",
    "        \n",
    "#         # Append the sentiment data to the list\n",
    "#         sentiment_data.append(ticker_sentiment)\n",
    "\n",
    "#     # Concatenate all the sentiment DataFrames into a single DataFrame\n",
    "#     sentiment_df = pd.concat(sentiment_data, ignore_index=True)\n",
    "    \n",
    "#     return sentiment_df\n",
    "\n",
    "# stock_headlines = generate_stock_df(top_100_tickers)\n",
    "# stock_headlines.to_csv(\"stock_headlines.csv\")\n",
    "# stock_headlines['published_date'] = pd.to_datetime(stock_headlines['published_date'])\n",
    "# stock_headlines['published_date'] = stock_headlines['published_date'].dt.tz_localize(None)\n",
    "# stock_headlines = stock_headlines[stock_headlines['published_date'] < pd.to_datetime('2025-02-11')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_date</th>\n",
       "      <th>headline</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-27 07:19:16.000</td>\n",
       "      <td>PRESS DIGEST- Wall Street Journal - Dec 27</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-19 00:58:19.000</td>\n",
       "      <td>PRESS DIGEST- Financial Times - Dec. 19</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-18 12:31:23.000</td>\n",
       "      <td>RPT-FOCUS-Goldman Sachs faces rocky exit from ...</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-05 05:14:57.902</td>\n",
       "      <td>Newscasts - Wall Street ends down as megacaps ...</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-04 21:00:38.184</td>\n",
       "      <td>Newscasts - U.S. Day Ahead:  Treasury yields r...</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2025-01-22 15:34:31.000</td>\n",
       "      <td>FACTBOX-List of UK competition regulator cases...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2025-01-22 11:49:59.000</td>\n",
       "      <td>UPDATE 3-UK boots out antitrust boss for faili...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2025-01-17 12:00:00.000</td>\n",
       "      <td>RPT-BREAKINGVIEWS-Mega-merger boom threatens a...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2025-01-16 22:00:00.000</td>\n",
       "      <td>BREAKINGVIEWS-Mega-merger boom threatens a sha...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2025-01-09 01:12:56.000</td>\n",
       "      <td>UPDATE 1-SoftBank and Arm weigh acquiring Ampe...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1701 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              published_date  \\\n",
       "0    2023-12-27 07:19:16.000   \n",
       "1    2023-12-19 00:58:19.000   \n",
       "2    2023-12-18 12:31:23.000   \n",
       "3    2023-12-05 05:14:57.902   \n",
       "4    2023-12-04 21:00:38.184   \n",
       "..                       ...   \n",
       "128  2025-01-22 15:34:31.000   \n",
       "129  2025-01-22 11:49:59.000   \n",
       "130  2025-01-17 12:00:00.000   \n",
       "131  2025-01-16 22:00:00.000   \n",
       "132  2025-01-09 01:12:56.000   \n",
       "\n",
       "                                              headline Ticker  \n",
       "0           PRESS DIGEST- Wall Street Journal - Dec 27   AAPL  \n",
       "1              PRESS DIGEST- Financial Times - Dec. 19   AAPL  \n",
       "2    RPT-FOCUS-Goldman Sachs faces rocky exit from ...   AAPL  \n",
       "3    Newscasts - Wall Street ends down as megacaps ...   AAPL  \n",
       "4    Newscasts - U.S. Day Ahead:  Treasury yields r...   AAPL  \n",
       "..                                                 ...    ...  \n",
       "128  FACTBOX-List of UK competition regulator cases...   GOOG  \n",
       "129  UPDATE 3-UK boots out antitrust boss for faili...   GOOG  \n",
       "130  RPT-BREAKINGVIEWS-Mega-merger boom threatens a...   GOOG  \n",
       "131  BREAKINGVIEWS-Mega-merger boom threatens a sha...   GOOG  \n",
       "132  UPDATE 1-SoftBank and Arm weigh acquiring Ampe...   GOOG  \n",
       "\n",
       "[1701 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "headlines_2022_2023 = pd.read_csv('headlines_top_100_stocks_2022_2023.csv')\n",
    "headlines_2024_1 = pd.read_csv('headlines_top_100_stocks_2024_first_half.csv')\n",
    "headlines_2024_2 = pd.read_csv('headlines_top_100_stocks_2024_second_half.csv')\n",
    "headlines_2025 = pd.read_csv('headlines_top_100_stocks_2025.csv')\n",
    "headlines_general_1 = pd.read_csv('headlines_top_100_stocks_2025.csv')\n",
    "headlines_general_2 = pd.read_csv('headlines_top_100_stocks_3perday.csv')\n",
    "combined = pd.concat([headlines_2022_2023,headlines_2024_1,headlines_2024_2,headlines_2025,headlines_general_1,headlines_general_2],axis=0)\n",
    "combined = combined.drop_duplicates()\n",
    "combined['cRIC'] = combined['cRIC'].str.replace('.O', '', regex=False)\n",
    "combined = combined.rename(columns={'versionCreated':'published_date',\n",
    "                                    'cRIC':'Ticker'})\n",
    "combined = combined.drop(columns=['storyId','sourceCode'])\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stock_data_returns = pd.read_csv('stock_data_daily_data.csv')\n",
    "stock_data_returns['Date'] = pd.to_datetime(stock_data_returns['Date'])\n",
    "\n",
    "lst = []\n",
    "prev_return_val = None  # Initialize previous return value\n",
    "\n",
    "# Iterate through each row in stock_headlines\n",
    "for row in combined.itertuples():\n",
    "    ticker = row.Ticker\n",
    "    published_date = pd.to_datetime(row.published_date)\n",
    "    \n",
    "    # Ensure the stock data date is localized correctly\n",
    "    stock_data_returns['Date'] = stock_data_returns['Date'].dt.tz_localize(None)\n",
    "    \n",
    "    # Get the current return value after the published date\n",
    "    return_val = stock_data_returns[stock_data_returns['Date'] > published_date][ticker].iloc[0]\n",
    "\n",
    "    if prev_return_val is not None:  # Check if we have a previous return value\n",
    "        # Calculate the absolute difference and compare it with 3 times the current return\n",
    "        if abs(return_val - prev_return_val) >= 2 * abs(return_val):\n",
    "            lst.append(1)\n",
    "        else:\n",
    "            lst.append(0)\n",
    "    else:\n",
    "        # If no previous return (first iteration), append -1 (or handle as needed)\n",
    "        lst.append(0)\n",
    "\n",
    "    # Update the previous return value for the next iteration\n",
    "    prev_return_val = return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['vol_label'] = lst # Balanced Dataframe\n",
    "stock_headlines = combined[['headline', 'vol_label']]\n",
    "stock_headlines = stock_headlines[~stock_headlines['headline'].str.contains('PRESS DIGEST', na=False)]\n",
    "# stock_headlines\n",
    "stock_headlines.to_csv(\"compiled_stocks_df_vol.csv\")\n",
    "# combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
