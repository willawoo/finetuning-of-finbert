{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA4265 Assignment 1: Fine-Tuning of FinBERT for Volatility Forecasting based on Mergers & Acquisitions-related Headlines\n",
    "\n",
    "Mergers and acquisitions play a crucial role in shaping a company's outlook, directly influencing investor sentiment and stock performance. Depending on whether investors perceive the news positively or negatively, they may adjust their trading decisions, causing fluctuations in stock prices and increased volatility. By forecasting these volatility shifts based on a single headline, investors with lower risk appetites may choose to sell their holdings or avoid purchasing the stock altogether, as higher volatility, while potentially offering greater returns, also brings the risk of significant losses. \n",
    "\n",
    "With that said, the goal of this assignment is to assess the impact of mergers and acquisitions-related headlines on the short-term, medium-term, and long-term volatility of the closing prices of a company’s ordinary shares, and this is done through the fine-tuning of a FinBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Extraction\n",
    "\n",
    "The following section describes the data extraction process and generation of the labelled dataframe. The tickers used for analysis are as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tickers used for Analysis\n",
    "top_tickers = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'BRK-B', 'V', 'PYPL', 'MA',\n",
    "    'BABA', 'JNJ', 'WMT', 'NFLX', 'DIS', 'NVDA', 'HD', 'PFE', 'ADBE', 'INTC', 'VZ', 'CSCO',\n",
    "    'GS', 'BA', 'CVX', 'UNH', 'KO', 'XOM', 'PEP', 'MCD', 'NKE', 'IBM', 'GM', 'GE', 'INTU',\n",
    "    'TXN', 'WFC', 'AMT', 'LMT', 'SPY', 'MS', 'GS', 'CAT', 'T', 'AXP', 'COP', 'MDT', 'MMM',\n",
    "    'UPS', 'PLD', 'AMD', 'F', 'PFE', 'GILD', 'C', 'STZ', 'LVS', 'MGM', 'ZTS', 'BMY', 'DELL',\n",
    "    'TMO', 'LLY', 'AMGN', 'DUK', 'STT', 'BA', 'AIG', 'CL', 'OXY', 'MU', 'SLB', 'HCA', 'HPE',\n",
    "    'LUV', 'UAL', 'SPG', 'CME', 'CSX', 'ETN', 'FIS', 'REGN', 'CVS', 'MDLZ', 'SYK', 'WBA',\n",
    "    'ZBRA', 'NOC', 'RMD', 'CHTR', 'TROW', 'RJF', 'FOXA', 'FOXF', 'HSY', 'KHC', 'NKE', 'TRV',\n",
    "    'ADP', 'VFC', 'MSCI', 'MKC', 'BAX', 'LRCX', 'TGT', 'FIS', 'RTX', 'CDNS', 'DG',\n",
    "    'LPLA', 'EXC', 'CTSH', 'MTB', 'STZ', 'WFC', 'COF', 'DHR', 'CCL', 'ECL', 'SPGI', \n",
    "    'TEL', 'WST', 'FMC', 'EL', 'VRSN', 'SWK', 'TMO', 'DE', 'ADBE', 'JNJ', 'SBUX', 'PFE',\n",
    "    'HPE', 'MMM', 'ABT', 'INTU', 'MELI', 'BA', 'GOOG'\n",
    "]\n",
    "\n",
    "# Addition of .O to indicate Ordinary Shares\n",
    "tickers_ordinary_share = [ticker + '.O' for ticker in top_tickers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1a: Data Extraction from Refinitiv Workspace and yfinance\n",
    "\n",
    "It is to note that the data obtained was sourced from Refinitiv Workspace, and the code to extract the dataframes were all copied and pasted from its in-built CodeBook. Following the data extraction, the stock prices were extracted with the help of yfinance libary. From these prices, the corresponding rolling volatilities were then computed.\n",
    "\n",
    "In this assignment, I utilised 3 different windows to represent various types of volatilities:\n",
    "\n",
    "1) 3-Day Rolling Window (Short-Term Volatility)\n",
    "2) 5-Day Rolling Window (Medium-Term Volatility)\n",
    "3) 10-Day Rolling Window (Long-Term Volatility)\n",
    "\n",
    "By using these 3 different windows, greater insight on the impact of headlines on the different terms of volatilities (Short-Term, Medium-Term and Long-Term) is hoped to be attained. Subsequently, based on the metrics used (F1 score), I can then decide which type of volatility (short-term, medium-term or long-term) is the easiest to predict based on the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import refinitiv.data as rd\n",
    "from refinitiv.data.content import news\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Refinitiv Workspace Session\n",
    "rd.open_session()\n",
    "\n",
    "# Extraction of Merger News Headlines data from Refinitiv Workspace\n",
    "start_dates = ['2022-01-01', '2024-01-01', '2024-07-01', '2025-01-01']\n",
    "end_dates = ['2023-12-31', '2024-06-30', '2024-12-31', '2025-02-11']\n",
    "csv_names = ['headlines_top_100_stocks_2022_2023.csv', 'headlines_top_100_stocks_2024_first_half.csv', \n",
    "            'headlines_top_100_stocks_2024_second_half.csv', 'headlines_top_100_stocks_2025.csv']\n",
    "for i in range(4):\n",
    "    compNews = pd.DataFrame()\n",
    "    start_date = start_dates[i]\n",
    "    end_date = end_dates[i]\n",
    "    csv_name = csv_names[i]\n",
    "    for ric in tickers_ordinary_share:\n",
    "        try:\n",
    "            cHeadlines = rd.news.get_headlines(\"Topic:MRG AND \"+ \"R:\" + ric + \" AND Language:LEN AND Source:RTRS\", start= start_date, \n",
    "                                            end = end_date, count = 100)\n",
    "            cHeadlines['cRIC'] = ric\n",
    "            \n",
    "            if len(compNews):\n",
    "                compNews = pd.concat([compNews,cHeadlines])\n",
    "            else:\n",
    "                compNews = cHeadlines\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    compNews.to_csv(csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_date</th>\n",
       "      <th>headline</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-27 07:19:16.000</td>\n",
       "      <td>PRESS DIGEST- Wall Street Journal - Dec 27</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-19 00:58:19.000</td>\n",
       "      <td>PRESS DIGEST- Financial Times - Dec. 19</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-18 12:31:23.000</td>\n",
       "      <td>RPT-FOCUS-Goldman Sachs faces rocky exit from ...</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-05 05:14:57.902</td>\n",
       "      <td>Newscasts - Wall Street ends down as megacaps ...</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-04 21:00:38.184</td>\n",
       "      <td>Newscasts - U.S. Day Ahead:  Treasury yields r...</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2025-01-22 15:34:31.000</td>\n",
       "      <td>FACTBOX-List of UK competition regulator cases...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2025-01-22 11:49:59.000</td>\n",
       "      <td>UPDATE 3-UK boots out antitrust boss for faili...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2025-01-17 12:00:00.000</td>\n",
       "      <td>RPT-BREAKINGVIEWS-Mega-merger boom threatens a...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2025-01-16 22:00:00.000</td>\n",
       "      <td>BREAKINGVIEWS-Mega-merger boom threatens a sha...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2025-01-09 01:12:56.000</td>\n",
       "      <td>UPDATE 1-SoftBank and Arm weigh acquiring Ampe...</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1701 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              published_date  \\\n",
       "0    2023-12-27 07:19:16.000   \n",
       "1    2023-12-19 00:58:19.000   \n",
       "2    2023-12-18 12:31:23.000   \n",
       "3    2023-12-05 05:14:57.902   \n",
       "4    2023-12-04 21:00:38.184   \n",
       "..                       ...   \n",
       "128  2025-01-22 15:34:31.000   \n",
       "129  2025-01-22 11:49:59.000   \n",
       "130  2025-01-17 12:00:00.000   \n",
       "131  2025-01-16 22:00:00.000   \n",
       "132  2025-01-09 01:12:56.000   \n",
       "\n",
       "                                              headline Ticker  \n",
       "0           PRESS DIGEST- Wall Street Journal - Dec 27   AAPL  \n",
       "1              PRESS DIGEST- Financial Times - Dec. 19   AAPL  \n",
       "2    RPT-FOCUS-Goldman Sachs faces rocky exit from ...   AAPL  \n",
       "3    Newscasts - Wall Street ends down as megacaps ...   AAPL  \n",
       "4    Newscasts - U.S. Day Ahead:  Treasury yields r...   AAPL  \n",
       "..                                                 ...    ...  \n",
       "128  FACTBOX-List of UK competition regulator cases...   GOOG  \n",
       "129  UPDATE 3-UK boots out antitrust boss for faili...   GOOG  \n",
       "130  RPT-BREAKINGVIEWS-Mega-merger boom threatens a...   GOOG  \n",
       "131  BREAKINGVIEWS-Mega-merger boom threatens a sha...   GOOG  \n",
       "132  UPDATE 1-SoftBank and Arm weigh acquiring Ampe...   GOOG  \n",
       "\n",
       "[1701 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combination of extracted data from Refinitiv API to form the overall fine-tuning test set\n",
    "\n",
    "import pandas as pd\n",
    "headlines_2022_2023 = pd.read_csv('headlines_top_100_stocks_2022_2023.csv')\n",
    "headlines_2024_1 = pd.read_csv('headlines_top_100_stocks_2024_first_half.csv')\n",
    "headlines_2024_2 = pd.read_csv('headlines_top_100_stocks_2024_second_half.csv')\n",
    "headlines_2025 = pd.read_csv('headlines_top_100_stocks_2025.csv')\n",
    "combined = pd.concat([headlines_2022_2023,headlines_2024_1,headlines_2024_2,headlines_2025],axis=0)\n",
    "\n",
    "combined = combined.drop_duplicates() # Avoiding any duplicate headlines\n",
    "\n",
    "combined['cRIC'] = combined['cRIC'].str.replace('.O', '', regex=False)\n",
    "combined = combined.rename(columns={'versionCreated':'published_date',\n",
    "                                    'cRIC':'Ticker'})\n",
    "combined = combined.drop(columns=['storyId','sourceCode'])\n",
    "combined.to_csv('combined_headlines_refinitiv.csv')\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  124 of 124 completed\n"
     ]
    }
   ],
   "source": [
    "# Second Data Extraction: Stock price data from yfinance library\n",
    "\n",
    "import yfinance as yf\n",
    "def fetch_stock_data(tickers, start_date=\"2020-01-01\", end_date=\"2025-02-14\"):\n",
    "    etf_data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "    return etf_data\n",
    "\n",
    "# Get the data for the ETFs\n",
    "stock_data = fetch_stock_data(top_tickers)\n",
    "\n",
    "# Calculate the daily price change (percentage change) for each stock ticker\n",
    "stock_price_changes = stock_data.pct_change().dropna(how='all')\n",
    "\n",
    "# Calculating the volatility over a rolling 3-day window\n",
    "rolling_volatility_3days = stock_price_changes.rolling(window=3).std().dropna(how='all')\n",
    "rolling_volatility_5days = stock_price_changes.rolling(window=5).std().dropna(how='all')\n",
    "rolling_volatility_10days = stock_price_changes.rolling(window=10).std().dropna(how='all')\n",
    "rolling_volatility_3days.to_csv(\"stock_data_vol_3d.csv\")\n",
    "rolling_volatility_5days.to_csv(\"stock_data_vol_5d.csv\")\n",
    "rolling_volatility_10days.to_csv(\"stock_data_vol_10d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Stock Volatility Dataset\n",
    "stock_data_vol_3d = pd.read_csv('stock_data_vol_3d.csv')\n",
    "stock_data_vol_5d = pd.read_csv('stock_data_vol_5d.csv')\n",
    "stock_data_vol_10d = pd.read_csv('stock_data_vol_10d.csv')\n",
    "headlines_df = pd.read_csv('combined_headlines_refinitiv.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Volatility Dataframe\n",
    "To generate the labels, it is to note that if the rolling volatility of the current share is more than or equal to twice of the previous rolling volatility, the label will be 1, otherwise it will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Function to merge volatility and headlines dataframe\n",
    "\n",
    "import pandas as pd\n",
    "def combining_fn(stock_data, headlines_df, window):\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "    lst = []\n",
    "    prev_return_vol = None  # Initialize previous return value\n",
    "\n",
    "    # Iterating through each row in stock_headlines\n",
    "    for row in headlines_df.itertuples():\n",
    "        ticker = row.Ticker\n",
    "        published_date = pd.to_datetime(row.published_date)\n",
    "        \n",
    "        # Ensure the stock data date is localized correctly\n",
    "        stock_data['Date'] = stock_data['Date'].dt.tz_localize(None)\n",
    "        \n",
    "        # Get the first volatility value after the published date\n",
    "        return_vol = stock_data[stock_data['Date'] > published_date][ticker].iloc[0]\n",
    "\n",
    "        if prev_return_vol is not None:  # Checking if we have a previous return value\n",
    "            \n",
    "            # Calculate the absolute difference and determine if current rolling volatility is >= 2 times the previous volatility\n",
    "            if abs(return_vol - prev_return_vol) / abs(prev_return_vol) >= 1:\n",
    "                lst.append(1)\n",
    "            else:\n",
    "                lst.append(0)\n",
    "        else:\n",
    "            # If no previous return (first iteration), append 0 (or handle as needed)\n",
    "            lst.append(0)\n",
    "\n",
    "        # Update the previous return value for the next iteration\n",
    "        prev_return_vol = return_vol\n",
    "        \n",
    "    headlines_df['vol_label'] = lst # Balanced Dataframe\n",
    "    stock_headlines = headlines_df[['headline', 'vol_label']]\n",
    "    \n",
    "    # Removal of any PRESS DIGEST article head since that is just the issue number (does not say anything about it; excluded from analysis)\n",
    "    stock_headlines = stock_headlines[~stock_headlines['headline'].str.contains('PRESS DIGEST', na=False)] \n",
    "    stock_headlines.to_csv(f\"compiled_df_{window}.csv\")\n",
    "    return stock_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming of dataframes for Short-Term, Medium-Term, and Long-Term Volatilities\n",
    "stock_headlines_3d = combining_fn(stock_data_vol_3d, headlines_df, window = '3d')\n",
    "stock_headlines_5d = combining_fn(stock_data_vol_5d, headlines_df, window = '5d')\n",
    "stock_headlines_10d = combining_fn(stock_data_vol_10d, headlines_df, window = '10d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RPT-FOCUS-Goldman Sachs faces rocky exit from ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Newscasts - Wall Street ends down as megacaps ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Newscasts - U.S. Day Ahead:  Treasury yields r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Newscasts - U.S. Morning Call: Elon Musk curse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newscasts - U.S. stocks little changed ahead o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>FACTBOX-List of UK competition regulator cases...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>UPDATE 3-UK boots out antitrust boss for faili...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>RPT-BREAKINGVIEWS-Mega-merger boom threatens a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>BREAKINGVIEWS-Mega-merger boom threatens a sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>UPDATE 1-SoftBank and Arm weigh acquiring Ampe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1501 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     RPT-FOCUS-Goldman Sachs faces rocky exit from ...      0\n",
       "1     Newscasts - Wall Street ends down as megacaps ...      0\n",
       "2     Newscasts - U.S. Day Ahead:  Treasury yields r...      0\n",
       "3     Newscasts - U.S. Morning Call: Elon Musk curse...      0\n",
       "4     Newscasts - U.S. stocks little changed ahead o...      0\n",
       "...                                                 ...    ...\n",
       "1496  FACTBOX-List of UK competition regulator cases...      0\n",
       "1497  UPDATE 3-UK boots out antitrust boss for faili...      0\n",
       "1498  RPT-BREAKINGVIEWS-Mega-merger boom threatens a...      0\n",
       "1499  BREAKINGVIEWS-Mega-merger boom threatens a sha...      0\n",
       "1500  UPDATE 1-SoftBank and Arm weigh acquiring Ampe...      0\n",
       "\n",
       "[1501 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the 3-Day, 5-Day and 10-Day Volatility forecast dataframes\n",
    "data_3d = pd.read_csv('compiled_df_3d.csv', encoding='utf-8', encoding_errors='ignore').drop(columns='Unnamed: 0').rename(columns = {'headline': 'text','vol_label':'label'})\n",
    "data_5d = pd.read_csv('compiled_df_5d.csv', encoding='utf-8', encoding_errors='ignore').drop(columns='Unnamed: 0').rename(columns = {'headline': 'text','vol_label':'label'})\n",
    "data_10d = pd.read_csv('compiled_df_10d.csv', encoding='utf-8', encoding_errors='ignore').drop(columns='Unnamed: 0').rename(columns = {'headline': 'text','vol_label':'label'})\n",
    "data_10d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1b: Handling of Imbalanced Dataframe\n",
    "\n",
    "Due to imbalanced datasets, using accuracy as a performance metrics is not suitable especially for long-term volatility which saw few headlines that were associated with high volatilities. As such, the F1 score was used as the performance metrics instead. To make the data more balanced, two methods (Back-Translation and Contextual Word Replacement) were done sequentially.\n",
    "\n",
    "#### Method 1: Back-Translation\n",
    "This method refers to the process of taking each headline and translating it to a random language. The translated headline will then be translated back to English, and this generates new texts which are typically distinct from the original. Both the original and newly-generated texts are then combined to form a more balanced dataframe.\n",
    "\n",
    "#### Method 2: Contextual Word Replacement\n",
    "In this method, the BERT model was used to replace words in each headline in the newly combined dataset. The BERT model was utilised over other potential models such as RoBERTa and DistilBERT since it balances computational efficiency with accuracy in word replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of relevant libraries and packages for Back-Translation and Word Replacement\n",
    "import random\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different languages that can be used\n",
    "lang_lst = [\"zh-CN\", \"fr\", \"ja\", \"de\", \"hi\", \"ta\", \"es\", \"id\", \"ko\", \n",
    "    \"ar\", \"pt\", \"ru\", \"it\", \"tr\", \"pl\", \"th\", \"ms\", \"bn\", \n",
    "    \"vi\", \"sv\"]\n",
    "\n",
    "def back_translate(text, languages, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        lang = random.choice(languages)  # Pick a random language\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=\"en\", target=lang).translate(text)\n",
    "            back_translated = GoogleTranslator(source=lang, target=\"en\").translate(translated)\n",
    "            if back_translated and back_translated.strip():\n",
    "                return back_translated  # Return valid translation\n",
    "        except Exception as e: # Handling errors\n",
    "            print(f\"Translation failed with {lang}, retrying ({attempt + 1}/{max_retries})... Error: {e}\") \n",
    "            time.sleep(2)  # Wait before retrying\n",
    "    return text  # Return original text if all retries fail\n",
    "\n",
    "def generate_word_replacement(sentence, tokenizer, model): # Generates the word replacement based on tokenizer\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    if not tokens:\n",
    "        return sentence  # Return original sentence if no tokens\n",
    "    idx = random.randint(0, len(tokens) - 1)\n",
    "    word_to_mask = tokens[idx]\n",
    "    masked_sentence = sentence.replace(word_to_mask, '[MASK]', 1)\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = outputs.logits\n",
    "    predicted_token_id = torch.argmax(predictions[0, idx]).item()\n",
    "    predicted_token = tokenizer.decode([predicted_token_id])\n",
    "    return masked_sentence.replace('[MASK]', predicted_token, 1)\n",
    "\n",
    "# Combination of back-translation and word replacement\n",
    "def augment_data_with_back_translation_and_word_replacement(data, tokenizer, model, languages=lang_lst, num_backtranslations=3, random_seed=43): \n",
    "    rare_class = data[data['label'] == 1]\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for text in rare_class[\"text\"]:\n",
    "        sampled_langs = random.sample(languages, num_backtranslations) # Randomly sample languages from language list; will vary every iteration\n",
    "        for lang in sampled_langs:\n",
    "            augmented_texts.append((back_translate(text, languages), 1))\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_texts, columns=[\"text\", \"label\"])\n",
    "    \n",
    "    augmented_sentences = [(generate_word_replacement(text, tokenizer, model), 1) for text in pd.concat([rare_class[\"text\"], augmented_df[\"text\"]])]\n",
    "    \n",
    "    augmented_replacement_df = pd.DataFrame(augmented_sentences, columns=[\"text\", \"label\"])\n",
    "    final_balanced_df = pd.concat([rare_class, augmented_df, augmented_replacement_df], ignore_index=True)\n",
    "    return final_balanced_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Generation of augmented data for Short-term, Medium-term and Long-term volatility dataframes\n",
    "final_augmented_df_3d = augment_data_with_back_translation_and_word_replacement(data_3d, tokenizer, model, num_backtranslations = 3)\n",
    "final_augmented_df_5d = augment_data_with_back_translation_and_word_replacement(data_5d, tokenizer, model, num_backtranslations = 6)\n",
    "final_augmented_df_10d = augment_data_with_back_translation_and_word_replacement(data_10d, tokenizer, model, num_backtranslations = 9)\n",
    "final_augmented_df_3d.to_csv('augmented_3d.csv')\n",
    "final_augmented_df_5d.to_csv('augmented_5d.csv')\n",
    "final_augmented_df_10d.to_csv('augmented_10d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of overall 'balanced' dataset\n",
    "final_augmented_df_3d = pd.read_csv('augmented_3d.csv').drop(columns='Unnamed: 0')\n",
    "final_augmented_df_5d = pd.read_csv('augmented_5d.csv').drop(columns='Unnamed: 0')\n",
    "final_augmented_df_10d = pd.read_csv('augmented_10d.csv').drop(columns='Unnamed: 0')\n",
    "\n",
    "# Extraction of 0-labels (majority class)\n",
    "data_3d_label0 = data_3d[data_3d['label']==0]\n",
    "data_5d_label0 = data_5d[data_5d['label']==0]\n",
    "data_10d_label0 = data_10d[data_10d['label']==0]\n",
    "\n",
    "# Merging of 0 and 1 labels to form combined dataset; shuffling of data is also done to mix the rows\n",
    "combined_3d = pd.concat([final_augmented_df_3d,data_3d_label0], axis=0, ignore_index=True).sample(frac=1, random_state=43).reset_index(drop=True)\n",
    "combined_5d = pd.concat([final_augmented_df_5d,data_5d_label0], axis=0, ignore_index=True).sample(frac=1, random_state=43).reset_index(drop=True)\n",
    "combined_10d = pd.concat([final_augmented_df_10d,data_10d_label0], axis=0, ignore_index=True).sample(frac=1, random_state=43).reset_index(drop=True)\n",
    "\n",
    "combined_3d.to_csv('compiled_df_3d_balanced.csv')\n",
    "combined_5d.to_csv('compiled_df_5d_balanced.csv')\n",
    "combined_10d.to_csv('compiled_df_10d_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-Tuning the Model\n",
    "\n",
    "In this assignment, I chose to use the FinBERT model. This is because through its strong understanding of financial texts and its ability to easily discern the contextual meaning of the headline, the FinBERT model can discern how mergers & acquisitions-related news influence market stability. Apart from that, its bidirectional attention will also allow it to work well with short, yet information-rich texts, where every word and its context matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import AdamW  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(43)\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = (\n",
    "    \"mps\" \n",
    "    if torch.backends.mps.is_available() \n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "device = torch.device(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_3d = pd.read_csv('compiled_df_3d_balanced.csv').drop(columns='Unnamed: 0')\n",
    "combined_5d = pd.read_csv('compiled_df_5d_balanced.csv').drop(columns='Unnamed: 0')\n",
    "combined_10d = pd.read_csv('compiled_df_10d_balanced.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation split: 80% train, 20% validation\n",
    "train_texts_3d, val_texts_3d, train_labels_3d, val_labels_3d = train_test_split(\n",
    "    combined_3d['text'].reset_index(drop=True),\n",
    "    combined_3d['label'].reset_index(drop=True),\n",
    "    test_size=0.2,\n",
    "    random_state=43\n",
    ")\n",
    "\n",
    "train_texts_5d, val_texts_5d, train_labels_5d, val_labels_5d = train_test_split(\n",
    "    combined_5d['text'].reset_index(drop=True),\n",
    "    combined_5d['label'].reset_index(drop=True),\n",
    "    test_size=0.2,\n",
    "    random_state=43\n",
    ")\n",
    "\n",
    "train_texts_10d, val_texts_10d, train_labels_10d, val_labels_10d = train_test_split(\n",
    "    combined_10d['text'].reset_index(drop=True),\n",
    "    combined_10d['label'].reset_index(drop=True),\n",
    "    test_size=0.2,\n",
    "    random_state=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer (FinBert)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Creation of Tokenizer class\n",
    "class Tokenizer(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        # Convert to list to ensure sequential indexing\n",
    "        self.texts = texts.tolist() if hasattr(texts, 'tolist') else list(texts)\n",
    "        self.labels = labels.tolist() if hasattr(labels, 'tolist') else list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "# Create datasets and data loaders for the different types of volatilities: short-term, medium-term, and long-term\n",
    "train_dataset_3d = Tokenizer(train_texts_3d, train_labels_3d, tokenizer)\n",
    "val_dataset_3d = Tokenizer(val_texts_3d, val_labels_3d, tokenizer)\n",
    "train_loader_3d = DataLoader(train_dataset_3d, batch_size=32, shuffle=True)\n",
    "val_loader_3d = DataLoader(val_dataset_3d, batch_size=32)\n",
    "\n",
    "train_dataset_5d = Tokenizer(train_texts_5d, train_labels_5d, tokenizer)\n",
    "val_dataset_5d = Tokenizer(val_texts_5d, val_labels_5d, tokenizer)\n",
    "train_loader_5d = DataLoader(train_dataset_5d, batch_size=32, shuffle=True)\n",
    "val_loader_5d = DataLoader(val_dataset_5d, batch_size=32)\n",
    "\n",
    "train_dataset_10d = Tokenizer(train_texts_10d, train_labels_10d, tokenizer)\n",
    "val_dataset_10d = Tokenizer(val_texts_10d, val_labels_10d, tokenizer)\n",
    "train_loader_10d = DataLoader(train_dataset_10d, batch_size=32, shuffle=True)\n",
    "val_loader_10d = DataLoader(val_dataset_10d, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FinBERT-based model class\n",
    "class VolClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super(VolClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('ProsusAI/finbert')\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)\n",
    "    \n",
    "# Initialize model\n",
    "model = VolClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Compute class weights (based on inverse frequency weighting) - Higher weight is assigned to the minority class\n",
    "# 3d - Short-Run (SR) volatility, 5d - Medium-Run (MR) volatility, 10d - Long-Run (LR) volatility\n",
    "\n",
    "class_counts_3d = torch.tensor([1335, 1328], dtype=torch.float)\n",
    "class_weights_3d = class_counts_3d.sum() / class_counts_3d\n",
    "class_weights_3d = class_weights_3d.to(device)\n",
    "\n",
    "class_counts_5d = torch.tensor([1421, 1120], dtype=torch.float)\n",
    "class_weights_5d = class_counts_5d.sum() / class_counts_5d\n",
    "class_weights_5d = class_weights_5d.to(device)\n",
    "\n",
    "class_counts_10d = torch.tensor([1454, 940], dtype=torch.float)\n",
    "class_weights_10d = class_counts_10d.sum() / class_counts_10d\n",
    "class_weights_10d = class_weights_10d.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Features\n",
    "During fine-tuning, the usage of a learning rate scheduler as well as gradual unfreezing of layers was adopted for the data. Incorporation of class weights using inverse frequency weights was also introduced to solve the issue of imbalanced dataframes.\n",
    "\n",
    "#### Method 1: Incorporation of Class Weights for Handling Slight Imbalances in Data\n",
    "With the data being slightly imbalanced even after the back-translation and word replacement steps, class weights are assigned using inverse frequency weights. This means greater weights are being assigned to the minority class, and smaller weights to the majority class.\n",
    "\n",
    "#### Method 2: Learning Rate Scheduler\n",
    "Given that FinBert requires smaller learning rates for fine-tuning (to avoid catastrophic forgetting of pre-trained knowledge), learning rate schedulers can help to extract meaningful signals from the space data within the financial texts. A linear decay scheduler with warmup was done in order to stabilise learning for the model. This helps the FinBERT model to better adapt to the headlines without abrupt weight updates. It also reduces overfitting risks by stabilizing learning in later epochs.\n",
    "\n",
    "#### Method 3: Gradual Unfreezing\n",
    "Gradual unfreezing is a layer-wise fine-tuning strategy where the model starts with only the final layers trainable, and progressively unfreezes earlier layers as training progresses. By training the model layer-by-layer, this allows the retention of generic linguistic knowledge while also adapting only relevant layers to the specific domain. Avoidance of catastrophic forgetting of pre-trained language patterns can be achieved, and this potentially improves overall stability of the model.\n",
    "\n",
    "#### Complementarity of Methods\n",
    "All 3 methods work well together to prevent overfitting and ensure proper training of the model. Gradual unfreezing helps to avoid losing the valuable representations learned during pre-training by gradually allowing the model to adjust the more task-specific layers without distorting the general-purpose language features in the early layers. This is complemented by the use of the Learning Rate Scheduler, where the rate of updates of the weights is faster. By using the scheduler, early exploration (rapid adjustments) will hence not disrupt the pre-trained knowledge, and this later ensures that the fine-tuning (small adjustments) process allows optimal model performance without overfitting. The incorporation of class weights also help to settle any potential imbalance within the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def fine_tuner(training_loader, validation_loader, class_weights, term):\n",
    "    \n",
    "    # Best parameters\n",
    "    optimizer = AdamW([\n",
    "        {'params': model.bert.parameters(), 'lr': 1e-5},\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "    ])\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights) # Application of class_weights\n",
    "\n",
    "    num_epochs = 3\n",
    "    num_training_steps = len(training_loader) * num_epochs\n",
    "    num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    def evaluate(model, val_loader):\n",
    "        model.eval()\n",
    "        total_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        print(\"\\nChecking validation dataset samples:\")\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            if i == 1:  # Print only the first batch\n",
    "                print(\"Sample input_ids:\", batch['input_ids'][0][:10])  # First 10 tokens of first sample\n",
    "                print(\"Sample label:\", batch['labels'][0].item())\n",
    "                break\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculation (saves memory and speeds up inference)\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                loss = loss_fn(outputs, labels) # Computing loss\n",
    "                total_loss += loss.item() # Accumulation of loss\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1) # Get predicted class\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_f1_score = f1_score(all_labels, all_preds, average='binary') # Computation of F1 score\n",
    "        val_loss = total_loss / len(val_loader)\n",
    "\n",
    "        return val_loss, val_f1_score\n",
    "\n",
    "    def train_phase(model, phase_name, num_epochs):\n",
    "        best_f1 = 0\n",
    "        print(f\"\\n======== Starting {phase_name} ========\")\n",
    "\n",
    "        # Print trainable layers\n",
    "        print(\"Trainable layers:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "\n",
    "        # Check model weights before training\n",
    "        print(\"\\nModel classifier weights before training:\", model.classifier.weight[:2])  # First two rows\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            all_preds, all_labels = [], []\n",
    "\n",
    "            for batch in tqdm(training_loader, desc=f\"Training {phase_name} Epoch {epoch+1}/{num_epochs}\"):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                loss.backward() # Backward pass\n",
    "                optimizer.step() # Update model parameters based on computed gradients\n",
    "                lr_scheduler.step() # Adjust the learning rate according to the scheduler\n",
    "\n",
    "            train_loss = total_loss / len(training_loader)\n",
    "            train_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "            val_loss, val_f1 = evaluate(model, validation_loader) # Evaluation of model on validation dataset\n",
    "\n",
    "            print(f\"{phase_name} - Epoch {epoch+1}/{num_epochs}: \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "            \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), f'best_model_{phase_name}_{term}.pt')\n",
    "            \n",
    "        # Check model weights after training\n",
    "        print(\"\\nModel classifier weights after training:\", model.classifier.weight[:2])  # First two rows\n",
    "\n",
    "    # Phase 1: Freeze all layers except the classifier head\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    train_phase(model, \"Phase1\", num_epochs)\n",
    "\n",
    "    # Phase 2: Unfreeze the last BERT layer\n",
    "    for param in model.bert.encoder.layer[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "    train_phase(model, \"Phase2\", num_epochs)\n",
    "\n",
    "    # Phase 3: Unfreeze the last 2 BERT layers\n",
    "    for i in range(-2, 0):\n",
    "        for param in model.bert.encoder.layer[i].parameters():\n",
    "            param.requires_grad = True\n",
    "    train_phase(model, \"Phase3\", num_epochs)\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Starting Phase1 ========\n",
      "Trainable layers:\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0029, -0.0144,  0.0242,  ...,  0.0139, -0.0106,  0.0178],\n",
      "        [-0.0042,  0.0065,  0.0119,  ..., -0.0198, -0.0064,  0.0336]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 1/3: 100%|██████████| 67/67 [05:34<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase1 - Epoch 1/3: Train Loss: 0.6570, Train F1: 0.6225, Val Loss: 0.6330, Val F1: 0.5774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 2/3: 100%|██████████| 67/67 [05:13<00:00,  4.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase1 - Epoch 2/3: Train Loss: 0.6506, Train F1: 0.6120, Val Loss: 0.6367, Val F1: 0.6112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 3/3: 100%|██████████| 67/67 [05:56<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase1 - Epoch 3/3: Train Loss: 0.6528, Train F1: 0.6278, Val Loss: 0.6259, Val F1: 0.5995\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0033, -0.0134,  0.0206,  ...,  0.0126, -0.0063,  0.0190],\n",
      "        [-0.0039,  0.0055,  0.0155,  ..., -0.0185, -0.0106,  0.0324]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "======== Starting Phase2 ========\n",
      "Trainable layers:\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0033, -0.0134,  0.0206,  ...,  0.0126, -0.0063,  0.0190],\n",
      "        [-0.0039,  0.0055,  0.0155,  ..., -0.0185, -0.0106,  0.0324]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 1/3: 100%|██████████| 67/67 [06:45<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase2 - Epoch 1/3: Train Loss: 0.6444, Train F1: 0.6317, Val Loss: 0.6259, Val F1: 0.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 2/3: 100%|██████████| 67/67 [05:31<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase2 - Epoch 2/3: Train Loss: 0.6485, Train F1: 0.6157, Val Loss: 0.6259, Val F1: 0.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 3/3: 100%|██████████| 67/67 [06:45<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase2 - Epoch 3/3: Train Loss: 0.6535, Train F1: 0.6259, Val Loss: 0.6259, Val F1: 0.5995\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0033, -0.0134,  0.0206,  ...,  0.0126, -0.0063,  0.0190],\n",
      "        [-0.0039,  0.0055,  0.0155,  ..., -0.0185, -0.0106,  0.0324]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "======== Starting Phase3 ========\n",
      "Trainable layers:\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0033, -0.0134,  0.0206,  ...,  0.0126, -0.0063,  0.0190],\n",
      "        [-0.0039,  0.0055,  0.0155,  ..., -0.0185, -0.0106,  0.0324]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 1/3: 100%|██████████| 67/67 [05:31<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase3 - Epoch 1/3: Train Loss: 0.6480, Train F1: 0.6259, Val Loss: 0.6259, Val F1: 0.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 2/3: 100%|██████████| 67/67 [04:33<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase3 - Epoch 2/3: Train Loss: 0.6495, Train F1: 0.6169, Val Loss: 0.6259, Val F1: 0.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 3/3: 100%|██████████| 67/67 [04:31<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 24482,  1011, 17235,  2850,  4160, 14572,  3020,  3805,  1997])\n",
      "Sample label: 0\n",
      "Phase3 - Epoch 3/3: Train Loss: 0.6510, Train F1: 0.6206, Val Loss: 0.6259, Val F1: 0.5995\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0033, -0.0134,  0.0206,  ...,  0.0126, -0.0063,  0.0190],\n",
      "        [-0.0039,  0.0055,  0.0155,  ..., -0.0185, -0.0106,  0.0324]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Short-Term Volatility Fine-Tuning\n",
    "fine_tuner(training_loader=train_loader_3d, validation_loader=val_loader_3d, class_weights=class_weights_3d, term='3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Starting Phase1 ========\n",
      "Trainable layers:\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0033, -0.0134,  0.0206,  ...,  0.0126, -0.0063,  0.0190],\n",
      "        [-0.0039,  0.0055,  0.0155,  ..., -0.0185, -0.0106,  0.0324]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 1/3: 100%|██████████| 64/64 [03:32<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase1 - Epoch 1/3: Train Loss: 0.6798, Train F1: 0.5213, Val Loss: 0.6483, Val F1: 0.6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 2/3: 100%|██████████| 64/64 [03:32<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase1 - Epoch 2/3: Train Loss: 0.6659, Train F1: 0.5719, Val Loss: 0.6486, Val F1: 0.5572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 3/3: 100%|██████████| 64/64 [03:33<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase1 - Epoch 3/3: Train Loss: 0.6606, Train F1: 0.5587, Val Loss: 0.6442, Val F1: 0.6200\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0061, -0.0111,  0.0236,  ...,  0.0128, -0.0070,  0.0192],\n",
      "        [-0.0011,  0.0032,  0.0125,  ..., -0.0187, -0.0100,  0.0322]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "======== Starting Phase2 ========\n",
      "Trainable layers:\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0061, -0.0111,  0.0236,  ...,  0.0128, -0.0070,  0.0192],\n",
      "        [-0.0011,  0.0032,  0.0125,  ..., -0.0187, -0.0100,  0.0322]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 1/3: 100%|██████████| 64/64 [03:56<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase2 - Epoch 1/3: Train Loss: 0.6679, Train F1: 0.5541, Val Loss: 0.6442, Val F1: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 2/3: 100%|██████████| 64/64 [03:56<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase2 - Epoch 2/3: Train Loss: 0.6570, Train F1: 0.5804, Val Loss: 0.6442, Val F1: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 3/3: 100%|██████████| 64/64 [04:03<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase2 - Epoch 3/3: Train Loss: 0.6653, Train F1: 0.5585, Val Loss: 0.6442, Val F1: 0.6200\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0061, -0.0111,  0.0236,  ...,  0.0128, -0.0070,  0.0192],\n",
      "        [-0.0011,  0.0032,  0.0125,  ..., -0.0187, -0.0100,  0.0322]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "======== Starting Phase3 ========\n",
      "Trainable layers:\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0061, -0.0111,  0.0236,  ...,  0.0128, -0.0070,  0.0192],\n",
      "        [-0.0011,  0.0032,  0.0125,  ..., -0.0187, -0.0100,  0.0322]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 1/3: 100%|██████████| 64/64 [04:24<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase3 - Epoch 1/3: Train Loss: 0.6610, Train F1: 0.5651, Val Loss: 0.6442, Val F1: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 2/3: 100%|██████████| 64/64 [04:43<00:00,  4.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase3 - Epoch 2/3: Train Loss: 0.6598, Train F1: 0.5586, Val Loss: 0.6442, Val F1: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 3/3: 100%|██████████| 64/64 [04:46<00:00,  4.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 12610,  1011, 13420, 12154, 12154, 24209,  2389,  9006,  2213])\n",
      "Sample label: 1\n",
      "Phase3 - Epoch 3/3: Train Loss: 0.6669, Train F1: 0.5558, Val Loss: 0.6442, Val F1: 0.6200\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0061, -0.0111,  0.0236,  ...,  0.0128, -0.0070,  0.0192],\n",
      "        [-0.0011,  0.0032,  0.0125,  ..., -0.0187, -0.0100,  0.0322]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Medium-Term Volatility Fine-Tuning\n",
    "fine_tuner(training_loader=train_loader_5d, validation_loader=val_loader_5d, class_weights=class_weights_5d, term='5d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Starting Phase1 ========\n",
      "Trainable layers:\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0061, -0.0111,  0.0236,  ...,  0.0128, -0.0070,  0.0192],\n",
      "        [-0.0011,  0.0032,  0.0125,  ..., -0.0187, -0.0100,  0.0322]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 1/3: 100%|██████████| 60/60 [04:16<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase1 - Epoch 1/3: Train Loss: 0.6808, Train F1: 0.5091, Val Loss: 0.6541, Val F1: 0.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 2/3: 100%|██████████| 60/60 [05:43<00:00,  5.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase1 - Epoch 2/3: Train Loss: 0.6733, Train F1: 0.5283, Val Loss: 0.6516, Val F1: 0.5193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase1 Epoch 3/3: 100%|██████████| 60/60 [05:14<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase1 - Epoch 3/3: Train Loss: 0.6638, Train F1: 0.5361, Val Loss: 0.6484, Val F1: 0.5181\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0063, -0.0109,  0.0246,  ...,  0.0154, -0.0060,  0.0185],\n",
      "        [-0.0008,  0.0030,  0.0115,  ..., -0.0214, -0.0109,  0.0329]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "======== Starting Phase2 ========\n",
      "Trainable layers:\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0063, -0.0109,  0.0246,  ...,  0.0154, -0.0060,  0.0185],\n",
      "        [-0.0008,  0.0030,  0.0115,  ..., -0.0214, -0.0109,  0.0329]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 1/3: 100%|██████████| 60/60 [03:52<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase2 - Epoch 1/3: Train Loss: 0.6562, Train F1: 0.5578, Val Loss: 0.6484, Val F1: 0.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 2/3: 100%|██████████| 60/60 [05:42<00:00,  5.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase2 - Epoch 2/3: Train Loss: 0.6618, Train F1: 0.5278, Val Loss: 0.6484, Val F1: 0.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase2 Epoch 3/3: 100%|██████████| 60/60 [05:36<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase2 - Epoch 3/3: Train Loss: 0.6614, Train F1: 0.5357, Val Loss: 0.6484, Val F1: 0.5181\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0063, -0.0109,  0.0246,  ...,  0.0154, -0.0060,  0.0185],\n",
      "        [-0.0008,  0.0030,  0.0115,  ..., -0.0214, -0.0109,  0.0329]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "======== Starting Phase3 ========\n",
      "Trainable layers:\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Model classifier weights before training: tensor([[-0.0063, -0.0109,  0.0246,  ...,  0.0154, -0.0060,  0.0185],\n",
      "        [-0.0008,  0.0030,  0.0115,  ..., -0.0214, -0.0109,  0.0329]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 1/3: 100%|██████████| 60/60 [05:18<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase3 - Epoch 1/3: Train Loss: 0.6541, Train F1: 0.5501, Val Loss: 0.6484, Val F1: 0.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 2/3: 100%|██████████| 60/60 [06:07<00:00,  6.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase3 - Epoch 2/3: Train Loss: 0.6534, Train F1: 0.5456, Val Loss: 0.6484, Val F1: 0.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Phase3 Epoch 3/3: 100%|██████████| 60/60 [04:25<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking validation dataset samples:\n",
      "Sample input_ids: tensor([  101, 10651,  1016,  1011,  2572,  2094,  2000,  9878,  8241, 12508])\n",
      "Sample label: 0\n",
      "Phase3 - Epoch 3/3: Train Loss: 0.6641, Train F1: 0.5260, Val Loss: 0.6484, Val F1: 0.5181\n",
      "\n",
      "Model classifier weights after training: tensor([[-0.0063, -0.0109,  0.0246,  ...,  0.0154, -0.0060,  0.0185],\n",
      "        [-0.0008,  0.0030,  0.0115,  ..., -0.0214, -0.0109,  0.0329]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Long-Term Volatility Fine-Tuning\n",
    "fine_tuner(training_loader=train_loader_10d, validation_loader=val_loader_10d, class_weights=class_weights_10d, term='10d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hyperparameter Tuning of the Results\n",
    "With multiple parameters available to vary to optimise the model, I tested multiple combinations of values of learning rates for the different layers within the FinBERT model. The following describes the parameters explored for the optimizer:\n",
    "- BERT Layer Learning Rate = [1e-5, 2e-5]\n",
    "- Classifier Learning Rate = [1e-5, 3e-5, 5e-5, 1e-3, 2e-3]\n",
    "- Weight Decay = [1e-5]\n",
    "\n",
    "It is to note that this document only contains the results to run the best model, and not the results of the different parameter combinations.\n",
    "\n",
    "The following are some insights that have been obtained while varying the parameters:\n",
    "\n",
    "1) Increasing classifier parameter resulted in worse performance of the model, with F1 scores decreasing in for all three models. Hence, the lower the Classifier Learning Rate, the better. However, beyond a certain point, decreasing the learning rate will result in slightly worse performance for the models as shown by Combinations 4 and 5.\n",
    "2) Decreasing the learning rate for BERT layers resulted in generally better performance of the model for all three terms of volatility.\n",
    "\n",
    "It is also to note that addition of weight decay = 1e-5 was attempted in Combination 1 for the short-term volatility. When the weight decay was included, the F1 score was lower at 0.4327 compared to the F1 score of 0.6030 when there was no weight decay. In view of this poor performance, weight decay was hence removed from consideration.\n",
    "\n",
    "The different combinations attempted in this paper are as such:\n",
    "\n",
    "Combination 1: BERT Learning Rate = 2e-5, Classification Learning Rate = 1e-3, Weight Decay = 0\n",
    "- Volatility (SR) F1 = 0.6030\n",
    "- Volatility (MR) F1 = 0.4110\n",
    "- Volatility (LR) F1 = 0.5176\n",
    "\n",
    "Combination 2: BERT Learning Rate = 2e-5, Classification Learning Rate = 2e-3, Weight Decay = 0\n",
    "- Volatility (SR) F1 = 0.4493\n",
    "- Volatility (MR) F1 = 0.4731\n",
    "- Volatility (LR) F1 = 0.4988\n",
    "\n",
    "Combination 3: BERT Learning Rate = 2e-5, Classification Learning Rate = 5e-4, Weight Decay = 0\n",
    "- Volatility (SR) F1 = 0.5683\n",
    "- Volatility (MR) F1 = 0.5521\n",
    "- Volatility (LR) F1 = 0.5122\n",
    "\n",
    "Combination 4: BERT Learning Rate = 1e-5, Classification Learning Rate = 3e-4, Weight Decay = 0\n",
    "- Volatility (SR) F1 = 0.5995\n",
    "- Volatility (MR) F1 = 0.6200\n",
    "- Volatility (LR) F1 = 0.5181\n",
    "\n",
    "Combination 5: BERT Learning Rate = 1e-5, Classification Learning Rate = 1e-4, Weight Decay = 0\n",
    "- Volatility (SR) F1 = 0.5986\n",
    "- Volatility (MR) F1 = 0.5757\n",
    "- Volatility (LR) F1 = 0.5434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation and Interpretation of Results\n",
    "\n",
    "The following describes some insights from the results, and some reasons for these differences:\n",
    "\n",
    "1) Prior to balancing, upon fine-tuning the model based on the original data, the F1 score for all three were 0.1 or less. Following the balancing of the data, the F1 score for all three were significantly higher, with all 3 exceeding 0.5, indicating that the model is better than a random guess.\n",
    "2) Among all three types of volatilities forecasted from the headlines, it appears that the medium-term volatility is most effective, with F1 scores reaching 0.62, followed by short-term which was approximately 0.6, and then long-term volatilities at 0.52. The poor performance of the long-term volatility fine-tuning could be attributed to the effect of the headlines being affected by other market-moving events such as interest rate changes and industrial trends, thereby resulting in false signals for volatility changes. Apart from that, mean reversion could have resulted in the stabilisation of prices over 10 days, hence reducing the distinct cause-effect relationships. The short-term volatility may not have performed as well potentially due to it being highly noisy since the headlines may simply cause a temporary spike, but this spike does not last beyond a day or two.\n",
    "\n",
    "### Reflection on Strengths and Weaknesses of Analysis\n",
    "\n",
    "#### Strengths\n",
    "The strengths of fine-tuning this model can be summarised below:\n",
    "\n",
    "1) Effective forecasting of the volatility of the stock based on the headlines is observed across all three terms, with the best performing model being the model forecasting the medium-term volatility (5-Day Rolling Window). This can be useful for fund managers who have lower risk-appetite clients or low-risk appetite traders who deal on a weekly basis. This fine-tuned model can guide their actions more effectively as they are no longer looking solely at the returns, but also at how much the prices will fluctuate.\n",
    "2) With the inclusion of data in 2023, headlines relating to the AI Boom would have been in the dataframe. This likely provided more data about high volatility in the dataset across all stocks, thereby enabling better fine-tuning as it ensures a more balanced dataset.\n",
    "3) The use of multiple methods to improve the model including the learning rate scheduler and gradual de-freezing of the FinBERT layers ensured that model was fine-tuned to the correct degree, thereby enabling a rise in F1 score following fine-tuning. The assignment of class weights to further handle potential imbalances in the data also aided in increasing the F1 score.\n",
    "\n",
    "#### Weaknesses\n",
    "However, some weaknesses observed is also stated below:\n",
    "\n",
    "1) Since the original dataframe was highly imbalanced with a lack of headlines that resulted in spikes for volatilities, back-translation and word replacements would only rephrase the headlines originally available. This means that overfitting could have occurred due to few distinct headlines being available in the first place.\n",
    "2) There was potentially lower accuracy in the word replacement step when preparing the dataset due to the use of the BERT model instead of RoBERTa which is more accurate, but is computationally more expensive.\n",
    "\n",
    "### Potential Future Work\n",
    "For further improvements, the following points could be considered to make the fine-tuning more effective:\n",
    "\n",
    "1) Extraction of more data for volatile stocks as only data from 2022 to February 2025 was extracted. To obtain more labelled data, the extraction of headlines during the COVID-19 period (2020 to 2021) could have been done as many of the stocks were extremely volatile during that period.\n",
    "2) Including more possible values of weight decay could be done to determine the optimal weight decay for a higher F1 score given that only one value was tested before weight decay was removed.\n",
    "3) More combinations of learning rates for the different layers of the BERT model could be considered for further fine-tuning. \n",
    "4) Trying different pre-trained models such as RoBERTa and BERT models may be done since they may yield different types of results.\n",
    "\n",
    "### Conclusion\n",
    "To summarise, this assignment presents the capabilities of fine-tuning the FinBERT model to suit the task at hand of predicting the changes in volatility of a stock given a merger-related headline. While the actual results may not sufficiently justify its effectiveness in forecasting the volatilities of the stock, further refinements can be made to increase the F1 score further as many other parameters such as the optimal number of epochs were not considered. Future research can explore these optimizations to enhance the model’s predictive power, making it a more reliable tool for investors navigating merger-related market movements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
